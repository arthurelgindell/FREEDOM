# MLX Inference Service Configuration

# Model Configuration
MODEL_PATH=/app/models/portalAI/UI-TARS-1.5-7B-mlx-bf16
MAX_TOKENS=2048
TEMPERATURE=0.7

# Server Configuration
HOST=0.0.0.0
PORT=8000

# Performance Tuning for Apple Silicon
# These can be adjusted based on your system specs
MLX_MEMORY_POOL_SIZE=16GB
MLX_UNIFIED_MEMORY=true

# Logging
LOG_LEVEL=INFO
STRUCTURED_LOGGING=true